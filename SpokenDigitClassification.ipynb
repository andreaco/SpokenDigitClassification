{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpokenDigitClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "014704a1892741a7bfc916cd635f620c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c8a4c8b576f141fa8e0e2487c9607844",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_450715d63cd04b6f8c8250ef76081d6e",
              "IPY_MODEL_af8aa32b32c2417d96c25d4efcc2b31c"
            ]
          }
        },
        "c8a4c8b576f141fa8e0e2487c9607844": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "450715d63cd04b6f8c8250ef76081d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e42bcb61a65b4544a3e0fe98ff9cfa5c",
            "_dom_classes": [],
            "description": "Loading audio samples:   3%",
            "_model_name": "IntProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5e6e883456e4b8a9cd8a639ac81cb68"
          }
        },
        "af8aa32b32c2417d96c25d4efcc2b31c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_076ccc7dd9544cb690311389bfaa1fa2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 67/2000 [00:08&lt;03:46,  8.54it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_831a448f92b24cbb95da209f357a151e"
          }
        },
        "e42bcb61a65b4544a3e0fe98ff9cfa5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5e6e883456e4b8a9cd8a639ac81cb68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "076ccc7dd9544cb690311389bfaa1fa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "831a448f92b24cbb95da209f357a151e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B-exD2Xwl2a",
        "colab_type": "text"
      },
      "source": [
        "# Spoken Digit Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KllSJGkZVfA",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN6hujgRtPoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import os\n",
        "import librosa\n",
        "import scipy as sp\n",
        "\n",
        "# Utility\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import Audio\n",
        "\n",
        "# Classifiers\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Scalers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6L3C0SzESw2",
        "colab_type": "text"
      },
      "source": [
        "## Dataset download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayoS7lOxXs56",
        "colab_type": "text"
      },
      "source": [
        "Here we are downloading the [dataset from the assignment](https://github.com/Jakobovski/free-spoken-digit-dataset) (N = 2000) along with a custom made dataset obtained from a subset of the [Google Speech Commands Dataset](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) (N = 23666) for testing purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph_o72b1tVm2",
        "colab_type": "code",
        "outputId": "ba2b4f7b-5cac-4ca5-e157-c19363d02334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "! git clone -q https://github.com/Jakobovski/free-spoken-digit-dataset\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"andreacoppola\"\n",
        "os.environ['KAGGLE_KEY'] = \"8d841874df721c79c79ada9beec21904\"\n",
        "!kaggle datasets download -d andreacoppola/googlespokendigits\n",
        "!unzip googlespokendigits.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading googlespokendigits.zip to /content\n",
            " 99% 660M/669M [00:16<00:00, 49.4MB/s]\n",
            "100% 669M/669M [00:16<00:00, 43.1MB/s]\n",
            "Archive:  googlespokendigits.zip\n",
            "  inflating: X_google_dataset.npy    \n",
            "  inflating: Y_google_dataset.npy    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgrA782qDh0j",
        "colab_type": "text"
      },
      "source": [
        "## Dataset import "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nLqh5FoMHB8",
        "colab_type": "text"
      },
      "source": [
        "The informations about the audio track are retrieved from the name using the following regexp\n",
        "```python\n",
        "'([0-9]+)_([a-z]+)_([0-9]+).wav'\n",
        "```\n",
        "With this we can retrieve from the 3 groups\n",
        "```python\n",
        "# Digit Label\n",
        "regexp.search(filename).group(1)\n",
        "# Spearker Name (maybe we'll need this later while analyzing the data?)\n",
        "regexp.search(filename).group(2)\n",
        "# File Index\n",
        "regexp.search(filename).group(3)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPhQ5Df0FMjV",
        "colab_type": "code",
        "outputId": "a3600303-ec7f-4e68-d341-83f70e6d7fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "014704a1892741a7bfc916cd635f620c",
            "c8a4c8b576f141fa8e0e2487c9607844",
            "450715d63cd04b6f8c8250ef76081d6e",
            "af8aa32b32c2417d96c25d4efcc2b31c",
            "e42bcb61a65b4544a3e0fe98ff9cfa5c",
            "a5e6e883456e4b8a9cd8a639ac81cb68",
            "076ccc7dd9544cb690311389bfaa1fa2",
            "831a448f92b24cbb95da209f357a151e"
          ]
        }
      },
      "source": [
        "# recordings directory\n",
        "directory = 'free-spoken-digit-dataset/recordings/'\n",
        "# regexp matching {digitLabel}_{speakerName}_{index}.wav\n",
        "regexp = re.compile('([0-9]+)_([a-z]+)_([0-9]+).wav') \n",
        "\n",
        "labels  = []\n",
        "audios  = []\n",
        "indexes = []\n",
        "sizes   = []\n",
        "names   = []\n",
        "\n",
        "# setup progress bar\n",
        "pbar = tqdm(desc='Loading audio samples')\n",
        "pbar.reset(total=len(os.listdir(directory))) \n",
        "\n",
        "for f in os.listdir(directory):\n",
        "  if f.endswith(\".wav\"): \n",
        "    labels.append(regexp.search(f).group(1))       # Take the first group (label)\n",
        "    names.append(regexp.search(f).group(2))        # Take the second groun (name)\n",
        "    indexes.append(int(regexp.search(f).group(3))) # Take the last group (index)\n",
        "    x, sr = librosa.load(directory+f, sr=None)     # Load audio file\n",
        "    sizes.append(x.size)\n",
        "    audios.append(x)                               # Put it in the list\n",
        "  pbar.update()                                    # updates progress bar\n",
        "pbar.refresh();\n",
        "\n",
        "# python lists to np.ndarray\n",
        "Y = np.asarray(labels)\n",
        "X = np.asarray(audios)\n",
        "I = np.asarray(indexes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "014704a1892741a7bfc916cd635f620c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', description='Loading audio samples', max=1, style=Progre…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTW0Do61cffx",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNGpOhq9cNlZ",
        "colab_type": "text"
      },
      "source": [
        "**Legend:**\n",
        "  - Y are the label for the assignment dataset\n",
        "  - X is the original dataset with tracks trimmed to be 1 second long at maximum to avoid unwantend noise\n",
        "  - L is an X version in which every audio track with less than 8000 samples is zero padded to be 8000 samples long, so each track is 1 sec long\n",
        "  - N and S are the names of the speakers and sizes\n",
        "  - X_GD and Y_GD are respectively the Google Dataset's _audio tracks_ and _labels_\n",
        "\n",
        "\n",
        "Each of the dataset test and train partitions can be obtaind by appending \"_test\" and \"_train\" on their name (except for the Google ones that are intended to be partitioned only if needed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Uk5AFVwzjH8",
        "colab_type": "text"
      },
      "source": [
        "### Assignment Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRy39tdHb_mC",
        "colab_type": "text"
      },
      "source": [
        "Here we are doing the trimming on X and extending the files that are over 8000 samples long to be 8000 sample long on L"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pf4WjRYdOsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Create an array (L) of empty audio tracks of 8000 samples\n",
        "For each audio track in X:\n",
        "  if the track is too long, trim it to be 1 second long (8000 samples)\n",
        "  Then fill the first\n",
        "'''\n",
        "# L is the trimmed and zero-padded dataset\n",
        "L = np.zeros((X.size, 8000))\n",
        "pbar = tqdm(desc='Zero Padding and trimming')\n",
        "pbar.reset(total=len(X))\n",
        "for i in range(len(X)):\n",
        "  if(X[i].size>8000): X[i] = X[i][0:8000] \n",
        "  L[i, 0:X[i].size] = X[i]                 \n",
        "  pbar.update();                             \n",
        "pbar.refresh();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ea_n8zeMLPV",
        "colab_type": "text"
      },
      "source": [
        "Here we're doing a train-test split using the conditions stated in the github repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1NMjl3kswRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_in_test  = I <= 4\n",
        "is_in_train = I >  4\n",
        "\n",
        "Y_test, Y_train = Y[is_in_test], Y[is_in_train]\n",
        "L_test, L_train = L[is_in_test], L[is_in_train]  # Trimmed and zero-padded dataset\n",
        "X_test, X_train = X[is_in_test], X[is_in_train]  # Original dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVc-mLjDZyl9",
        "colab_type": "text"
      },
      "source": [
        "If needed we can extract the speaker name and sizes too.  \n",
        "_(NB: the sizes was used for analysis purposes on the dataset and don't necessarily reflect the sizes used after the preprocessing step)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5BJTfqvb0CL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = np.asarray(names)\n",
        "S = np.asarray(sizes)\n",
        "\n",
        "N_test, N_train = N[is_in_test], N[is_in_train]\n",
        "S_test, S_train = S[is_in_test], S[is_in_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4huRZ4vzoic",
        "colab_type": "text"
      },
      "source": [
        "### Google Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWM13ERH9Db4",
        "colab_type": "text"
      },
      "source": [
        "Extraction of the spoken digits from the Google Dataset (please be careful, the label here is an integer and not a string!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TnQjYDS9n93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_GD = np.load('X_google_dataset.npy', allow_pickle=True)\n",
        "Y_GD = np.load('Y_google_dataset.npy', allow_pickle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34AWUjUc6vYC",
        "colab_type": "text"
      },
      "source": [
        "## Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWDCL45a3yyW",
        "colab_type": "text"
      },
      "source": [
        "This is a series of functions we used to do classification tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iSsethA6oe8",
        "colab_type": "text"
      },
      "source": [
        "### Windowing Infos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm48vWFR6UWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "  Informations about windowing in order to not repeat them \n",
        "  everytime we compute a feature that needs to know the number and type of windows\n",
        "'''\n",
        "Fs=8000\n",
        "win_length = int(np.floor(0.01 * Fs)) \n",
        "hop_size = int(np.floor(0.0075 * Fs))\n",
        "audio_ref = L_test[1]\n",
        "win_number = int(np.floor((audio_ref.shape[0] - win_length) / hop_size))\n",
        "window = sp.signal.get_window(window='hanning', Nx=win_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bJ7qpQMbNC_",
        "colab_type": "text"
      },
      "source": [
        "### MFCC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTJnaTXIbLBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_mfcc(audio, fs, n_mfcc):\n",
        "    # Compute the spectrogram of the audio signal\n",
        "    X = np.abs(librosa.stft(\n",
        "        audio,\n",
        "        window='hamming',\n",
        "        n_fft=1024,\n",
        "        hop_length=512,)\n",
        "        )\n",
        "    \n",
        "    # Find the weights of the mel filters\n",
        "    mel = librosa.filters.mel(\n",
        "        sr=fs,\n",
        "        n_fft=1024,\n",
        "        n_mels=40,\n",
        "        fmin=133,\n",
        "        fmax=4000\n",
        "    )\n",
        "\n",
        "    # Apply the filters to spectrogram\n",
        "    melspectrogram = np.dot(mel, X)\n",
        "\n",
        "    # Logarithm\n",
        "    log_melspectrogram = np.log10(melspectrogram + 1e-16)\n",
        "    \n",
        "    # DCT\n",
        "    mfcc = sp.fftpack.dct(log_melspectrogram, axis=0, norm='ortho')[1:n_mfcc+1]\n",
        "\n",
        "    return mfcc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbYbnpev23_s",
        "colab_type": "text"
      },
      "source": [
        "### Zero Crossing Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64zIrdME2-nW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_zcr(audio, fs):\n",
        "  zcr = np.zeros(win_number)\n",
        "  for n in np.arange(win_number):\n",
        "    frame = audio[n * hop_size : n * hop_size + win_length]\n",
        "    frame_win = frame * window\n",
        "    win_sign = np.sign(frame_win)\n",
        "    N = np.shape(frame_win)[0]\n",
        "    sign_diff = np.abs(win_sign[: -1] - win_sign[1 :])\n",
        "    zcr[n] = (fs / 2*N) * np.sum(sign_diff) \n",
        "  return zcr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hhoea4r3KK6",
        "colab_type": "text"
      },
      "source": [
        "### Spectral Decrease"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxUd7WOq3IUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_specdec(audio):\n",
        "  spectral_decrease = np.zeros(win_number)\n",
        "  for n in np.arange(win_number):\n",
        "    frame = audio[n * hop_size : n * hop_size + win_length]\n",
        "    frame_win = frame * window\n",
        "    spec = np.fft.fft(frame_win)\n",
        "    mul_fact = 1 / np.sum(np.abs(spec[1:]))\n",
        "    num = np.abs(spec[1:]) - np.tile(A = np.abs(spec[0]), reps=len(spec))[1:]\n",
        "    den = np.arange(1, len(spec)) - 1\n",
        "    spectral_decrease[n] = mul_fact * np.sum(num / den)\n",
        "  return spectral_decrease"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScFeE11S3TTZ",
        "colab_type": "text"
      },
      "source": [
        "### Spectral Centroid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TvVmeBU3SmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_speccentr(audio):\n",
        "  speccentr = np.zeros(win_number)\n",
        "  for n in np.arange(win_number):\n",
        "    frame = audio[n * hop_size : n * hop_size + win_length]\n",
        "    frame_win = frame * window\n",
        "    spec = np.fft.fft(frame_win)\n",
        "    k_axis = np.arange(spec.shape[0])\n",
        "    speccentr[n] = np.sum(np.transpose(k_axis)*np.abs(spec)) / np.sum(np.abs(spec))\n",
        "  return speccentr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4tcqY9mqp_V",
        "colab_type": "text"
      },
      "source": [
        "## Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uimMGG-5zUXx",
        "colab_type": "text"
      },
      "source": [
        "### Functions for classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi3bz9hQ5zKb",
        "colab_type": "text"
      },
      "source": [
        "This function encapsulates the previous functions to compare the performances during the classification step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t64EJMJKm0Ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_extraction(data, feat_type):\n",
        "  size = len(data)\n",
        "  \n",
        "  pbar = tqdm(desc='Feature extracion... [{}]'.format(feat_type))\n",
        "  pbar.reset(total=len(data))\n",
        "\n",
        "  Fs = 8000\n",
        "  features = None\n",
        "  Z= data.copy()\n",
        "  threshold_val = 0.1\n",
        "  for i in range(len(Z)):\n",
        "    Z_max = np.max(Z[i])\n",
        "    Z_min = np.min(Z[i])\n",
        "    conditions =  np.logical_and (Z[i]<Z_max*threshold_val, Z[i]>Z_min*threshold_val)\n",
        "    Z[i][conditions] = 0 \n",
        "   \n",
        "  \n",
        "  if(feat_type == \"mfcc20\"):\n",
        "    n_mfcc = 20\n",
        "    features = np.zeros((size, n_mfcc))\n",
        "    for i in range(size):\n",
        "      mfcc = compute_mfcc(data[i], Fs, n_mfcc)\n",
        "      features[i, :] = np.mean(mfcc, axis=1)\n",
        "      pbar.update()\n",
        "    pbar.refresh()\n",
        "  if(feat_type == \"mfcc13\"):\n",
        "    n_mfcc = 13\n",
        "    features = np.zeros((size, n_mfcc))\n",
        "    for i in range(size):\n",
        "      mfcc = compute_mfcc(data[i], Fs, n_mfcc)\n",
        "      features[i, :] = np.mean(mfcc, axis=1)\n",
        "      pbar.update()\n",
        "    pbar.refresh()\n",
        "  if(feat_type == \"zcr\"):\n",
        "    features = np.zeros((size, win_number))\n",
        "    for i in range(size):\n",
        "      features[i, :] = compute_zcr(Z[i], Fs)\n",
        "      pbar.update()\n",
        "    pbar.refresh()\n",
        "  if(feat_type == \"specdec\"): \n",
        "    features = np.zeros((size, win_number))\n",
        "    for i in range(size):\n",
        "      features[i, :] = compute_specdec(data[i])\n",
        "      pbar.update()\n",
        "    pbar.refresh()\n",
        "  if(feat_type == \"speccentr\"): \n",
        "      features = np.zeros((size, win_number))\n",
        "      for i in range(size):\n",
        "        features[i, :] = compute_speccentr(data[i])\n",
        "        pbar.update()\n",
        "      pbar.refresh()\n",
        "  if(feat_type == \"zcr+mfcc20\"):\n",
        "      n_mfcc = 20\n",
        "      features = np.zeros((size, n_mfcc + win_number +2)) \n",
        "      for i in range(size):\n",
        "        features[i,0:n_mfcc] = np.mean(compute_mfcc(data[i], Fs, n_mfcc), axis=1)\n",
        "        features[i,n_mfcc+1:-1] = compute_zcr(Z[i], Fs)\n",
        "        pbar.update()\n",
        "      pbar.refresh()  \n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWQXm5Z6EvrU",
        "colab_type": "text"
      },
      "source": [
        "Metrics to be used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FWNV7hLCyxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_metrics (y_pred, y_test):\n",
        "  acc = accuracy_score(y_pred, y_test)\n",
        "  cm  = confusion_matrix(y_pred, y_test)\n",
        "\n",
        "  return [(\"Accuracy\",acc), (\"ConfusionMatrix\",cm)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFEqcf3DDoWc",
        "colab_type": "text"
      },
      "source": [
        "### Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2XaAWvyDjcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This are the features that we want to extract using the feature_extraction function\n",
        "feature_methods = [\"mfcc13\", \"mfcc20\"]\n",
        "\n",
        "classifiers = []\n",
        "metrics_dict = {} # this will contain the metrics for each classifier and each feature\n",
        "                  # EX: for classifier \"foo\" and feature \"bar\" metrics_dict[\"foo\"][\"bar\"]\n",
        "\n",
        "# Feature Extraction and Scaling\n",
        "train_features_dict = {}\n",
        "\n",
        "scalers_dict = {}\n",
        "for fm in feature_methods:\n",
        "  train_feature = feature_extraction(X_train, fm)\n",
        "\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(train_feature)\n",
        "  scaler.transform(train_feature)\n",
        "\n",
        "  train_features_dict[fm] = train_feature\n",
        "  scalers_dict[fm] = scaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPxDIjTTAZal",
        "colab_type": "text"
      },
      "source": [
        "### Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnLQ6LBTcAFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifier training\n",
        "SVM_dict = {}\n",
        "for fm in feature_methods:\n",
        "  clf = svm.SVC()\n",
        "  clf.fit(train_features_dict[fm], Y_train)\n",
        "  SVM_dict[fm] = clf\n",
        "\n",
        "\n",
        "# Test and metric computation\n",
        "SVM_metrics_dict = {}\n",
        "for fm in feature_methods:\n",
        "  # Feature extraction\n",
        "  test_features = feature_extraction(X_test, fm)\n",
        "  # Feature scaling\n",
        "  scalers_dict[fm].transform(test_features)\n",
        "\n",
        "  # Confusion Matrix Plot\n",
        "  disp = plot_confusion_matrix(SVM_dict[fm], test_features, Y_test)\n",
        "  disp.ax_.set_title(fm)\n",
        "  plt.show()\n",
        "\n",
        "  # Compute metrics\n",
        "  y_pred = SVM_dict[fm].predict(test_features)\n",
        "  metrics = compute_metrics (y_pred, Y_test)\n",
        "  SVM_metrics_dict[fm] = metrics\n",
        "\n",
        "classifiers.append('SVM')\n",
        "metrics_dict['SVM'] = SVM_metrics_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VEynSXsAczO",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbXl8i1PAxN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifier training\n",
        "RFC_dict = {}\n",
        "for fm in feature_methods:\n",
        "  clf = RandomForestClassifier(n_estimators=100, max_features=\"sqrt\")\n",
        "  clf.fit(train_features_dict[fm], Y_train)\n",
        "  RFC_dict[fm] = clf\n",
        "\n",
        "\n",
        "# Test and metric computation\n",
        "RFC_metrics_dict = {}\n",
        "for fm in feature_methods:\n",
        "  # Feature extraction\n",
        "  test_features = feature_extraction(X_test, fm)\n",
        "  # Feature scaling\n",
        "  scalers_dict[fm].transform(test_features)\n",
        "\n",
        "  # Confusion Matrix Plot\n",
        "  disp = plot_confusion_matrix(RFC_dict[fm], test_features, Y_test)\n",
        "  disp.ax_.set_title(fm)\n",
        "  plt.show()\n",
        "\n",
        "  y_pred = RFC_dict[fm].predict(test_features)\n",
        "  metrics = compute_metrics (y_pred, Y_test)\n",
        "  RFC_metrics_dict[fm] = metrics\n",
        "\n",
        "classifiers.append('RFC')\n",
        "metrics_dict['RFC'] = SVM_metrics_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utzHnVWjBMLS",
        "colab_type": "text"
      },
      "source": [
        "### Simple Neural Networks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfPZMcYvBSnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature that we want to use\n",
        "fm = \"mfcc20\"\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "ohe_y_train = ohe.fit_transform(Y_train.astype(int).reshape(-1, 1)).toarray()\n",
        "ohe_y_test  = ohe.fit_transform(Y_test .astype(int).reshape(-1, 1)).toarray()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=20, activation='relu'))\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_feature, ohe_y_train, epochs=100, batch_size=64)\n",
        "\n",
        "# Feature extraction\n",
        "test_features = feature_extraction(X_test, fm)\n",
        "# Feature scaling\n",
        "scalers_dict[fm].transform(test_features)\n",
        "\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "#Converting predictions to label\n",
        "pred = list()\n",
        "for i in range(len(y_pred)):\n",
        "    pred.append(np.argmax(y_pred[i]))\n",
        "#Converting one hot encoded test label to label\n",
        "test = list()\n",
        "for i in range(len(ohe_y_test)):\n",
        "    test.append(np.argmax(ohe_y_test[i]))\n",
        "\n",
        "plt.imshow(confusion_matrix(pred, test))\n",
        "metrics = compute_metrics(pred, test)\n",
        "\n",
        "\n",
        "metrics_dict['NN'] = {fm : metrics}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iOgvxC5GZti",
        "colab_type": "text"
      },
      "source": [
        "## Performance Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD6U5gIVGb1A",
        "colab_type": "text"
      },
      "source": [
        "Here we can evaluate the performance of the classifiers using the metrics previously computed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw5E5D0kGrFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for c in classifiers:\n",
        "  print(\"========\", c, \"========\")\n",
        "  for fm in feature_methods:\n",
        "    if metrics_dict.get(c) != None and metrics_dict.get(c).get(fm) != None:\n",
        "      print(\"#\", fm, \"#\")\n",
        "      for m in metrics_dict[c][fm]:\n",
        "        print(\"[{}]\".format(m[0]))\n",
        "        print(\"{}\".format(m[1]))\n",
        "  print(\"===========================\\n\")\n",
        "\n",
        "metrics_dict['NN']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFhB1sClJTMv",
        "colab_type": "text"
      },
      "source": [
        "## Style"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrXMakXHJceG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib as mpl\n",
        "COLOR = 'darkgrey'\n",
        "mpl.rcParams['text.color'] = COLOR\n",
        "mpl.rcParams['axes.labelcolor'] = COLOR\n",
        "mpl.rcParams['xtick.color'] = COLOR\n",
        "mpl.rcParams['ytick.color'] = COLOR"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}